\section{Description des classifieurs utilisés}

	Il nous était possible d'utiliser deux sortes d'apprentissage sur nos données. L'apprentissage non-supervisé apprend uniquement à partir des attributs, sans tenir compte des labels. Cela permet par exemple de faire du clustering pour regrouper les données en différentes classes. Nous avons appliqué cette méthode d'apprentissage en utilisant l'algorithme des k-moyennes sur nos données.\\

	L'apprentissage supervisé apprend en se basant sur les attributs mais en utilisant également les labels que nous connaissons. Cette technique d'apprentissage paraît donc plus adaptée dans notre cas. Différentes méthodes existent, nous avons choisis de nous concentrer sur les SVM (\emph{Support Vector Machines} ou séparateurs à vaste marge) pour réaliser notre classification en apprentissage supervisé. Ils sont souvent utilisés pour résoudre des problèmes de discrimination, c'est-à-dire savoir à quelle classe appartient un échantillon, et c'est exactement le problème auquel nous sommes confrontés. Le SVM va chercher à tracer une limite entre les classes des données d'apprentissage en maximisant la marge entre cette limite et chaque classe. Les classes de nouvelles données seront ensuite attribuées selon le côté du quel se trouvent les nouvelles données. La limite entre les classes peut-être en ligne droite, dans le cas d'un SVM linéaire, ou plus compliquée dans le cas d'un SVM avec noyau.\\

	Dans un premier temps nous avons travaillé sur deux classes, ce qui est le problème le plus simple à résoudre avec des SVM. Nous avons pour cela réalisé deux SVM, un svm linéaire et un svm gaussien. Nous avons effectué de la validation croisée afin d'identifier les hyper-paramètres (C pour le svm linéaire, et C et sigma pour le svm gaussien).

	Dans un second temps nous avons travaillé sur un jeu de données contenant 4 classes et des données \emph{unlabeled} représentant une cinquième classe. Les données unlabeled ont été retirées de l'ensemble de l'apprentissage car elle pouvait correspondre à n'importe quelle classe et donc perturber le SVM.
	Nous avons ensuite utilisé deux méthodes permettant de faire de la classification avec plus de deux classes : 

	\begin{itemize}
	\item La méthode \emph{one-versus-all} consiste à construire M classifieurs binaires en attribuant le label 1 aux échantillons de l'une des classes et le label -1 à toutes les autres. En phase de test, le classifieur donnant la marge la plus élevée remporte le vote.
	\item La méthode \emph{one-versus-one} consiste à construire M(M-1)/2 classifieurs binaires en confrontant chacune des M classes. En phase de test, l'échantillon à classer est analysé par chaque classifieur et un vote majoritaire permet de déterminer sa classe.\\
	\end{itemize}


\section{Résultats obtenus}

	\subsection{Classification sur deux classes}
		\subsubsection{K-moyennes}
		\subsubsection{SVM linéaire}
		\subsubsection{SVM gaussien}

	\subsection{Classification sur 4 classes}
		\subsubsection{SVM one-versus-one}
		\subsubsection{SVM one-versus-all}